# -*- coding: utf-8 -*-
"""incremental_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nK0Yi096gW0GuHHuo_A2k_2GROtdt9AG

config
"""

Facnet_config = {
    "Epochs": 5,                  # Number of epochs per increment
    "learning_rate": 10**-4,       # Learning rate for optimizer
    "epsilon": 1e-8,               # Epsilon to avoid zero in denominator in the loss function
    "alpha": 1,                    # Margin alpha for loss
    "nb_clusters": [2, 64],        # Number of clusters per class [min_clusters, max_clusters]
    "M": 16,                       # Number of clusters in a mini-batch
    "D": 8,                        # Number of samples per cluster in a mini-batch
    "K": 15,                       # K nearest neighbors for KNN classification
    "L": 3,                        # Number of nearest clusters used for classification
    "nb_batches": 10,              # Number of batches per epoch
    "list_classes": [],            # List of classes in the current increment (e.g., [0, 1] for the first increment)
    "batch_size": 64,              # Batch size
    "optimizer_flag": 'Adam',      # Optimizer
    "width": 2,                    # Width of the wide residual block in the WRN model
}

"""class batch"""

# -*- coding: utf-8 -*-
"""
Created on Sat Jan 25 19:16:10 2020

@author: abdelhamid
"""
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances

class class_batch():
    def __init__(self, clusters_centers,clusters_labels,samples_indices):

        self.M                 = Facnet_config['M']
        self.D                 = Facnet_config['D']

        self.clusters_centers  = clusters_centers
        self.clusters_labels   = clusters_labels
        self.samples_indices   = samples_indices
        self.P_dis             = euclidean_distances(clusters_centers,clusters_centers)
        self.S_dis             = np.argsort(self.P_dis,axis=1)[:,1:]

    def construct_batch(self,inds1):
        batch_inds  = []
        batch_label = []

        if self.samples_indices[inds1].shape[0]<self.D:
            batch_inds.append(self.samples_indices[inds1])
            batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds1],axis=0),self.samples_indices[inds1].shape[0],axis=0))
        else:
            batch_inds.append(self.samples_indices[inds1][np.random.randint(low=0, high=self.samples_indices[inds1].shape[0], size=self.D)])
            batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds1],axis=0),self.D,axis=0))

        clusters_inds = self.choose_nearest(inds1)
        for i in range(clusters_inds.shape[0]):
            inds2 = clusters_inds[i]
            if self.samples_indices[inds2].shape[0]<self.D:
                batch_inds.append(self.samples_indices[inds2])
                batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds2],axis=0),self.samples_indices[inds2].shape[0],axis=0))
            else:
                batch_inds.append(self.samples_indices[inds2][np.random.randint(low=0, high=self.samples_indices[inds2].shape[0], size=self.D)])
                batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds2],axis=0),self.D,axis=0))

        batch_inds      = np.concatenate(batch_inds,axis=0)
        batch_label     = np.concatenate(batch_label,axis=0)
        chosen_clusters = np.unique(batch_label, axis=0)
        return batch_inds,batch_label,chosen_clusters

    def choose_nearest(self,inds1):

        pos = int((2/3)*self.M)
        neg = int((1/3)*self.M)

        labels  = self.clusters_labels[self.S_dis[inds1,:],0]
        test    = np.where(labels[:self.M]!=self.clusters_labels[inds1,0])[0].shape[0]

        if test>neg:
            clusters_inds = self.S_dis[inds1,:self.M]
        else:
            inds_eq   = np.where(labels==self.clusters_labels[inds1,0])[0]
            inds_ineq = np.where(labels!=self.clusters_labels[inds1,0])[0]

            clusters_inds = np.concatenate([inds_eq[:pos],inds_ineq[:neg]],axis=0)

        return clusters_inds

"""cluster training"""

import numpy as np
from sklearn.cluster import KMeans
def cluster_training(train_features, train_labels):
    list_classes = Facnet_config['list_classes']
    nb_clusters = Facnet_config['nb_clusters'][0]  # Assuming fixed number of clusters per class

    clusters_labels = []
    samples_indices = []
    clusters_centers = []
    sigma = 0

    for c in list_classes:
        inds_c = np.where(train_labels == c)[0]
        class_features = train_features[inds_c]

        kmeans = KMeans(n_clusters=nb_clusters, random_state=0).fit(class_features)
        sigma += kmeans.inertia_

        for i in range(nb_clusters):
            clusters_centers.append(kmeans.cluster_centers_[i])
            clusters_labels.append(np.array([c, i]))  # Class and cluster number
            samples_indices.append(inds_c[np.where(kmeans.labels_ == i)[0]])

    clusters_centers = np.stack(clusters_centers, axis=0)
    clusters_labels = np.stack(clusters_labels, axis=0)
    sigma = sigma / (nb_clusters * len(list_classes))

    return clusters_centers, clusters_labels, samples_indices, sigma

"""create_model"""

# -*- coding: utf-8 -*-
"""
Created on Wed Sep  4 22:11:19 2019

@author: Abdelhamid
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

def conv3x3(i_c, o_c, stride=1):
    return nn.Conv2d(i_c, o_c, 3, stride, 1, bias=False)


class BatchNorm2d(nn.BatchNorm2d):
    def __init__(self, channels, momentum=1e-3, eps=1e-3):
        super().__init__(channels)
        self.update_batch_stats = True

    def forward(self, x):
        if self.update_batch_stats:
            return super().forward(x)
        else:
            return nn.functional.batch_norm(
                x, None, None, self.weight, self.bias, True, self.momentum, self.eps
            )


def relu():
    return nn.LeakyReLU(0.1)

class residual(nn.Module):
    def __init__(self, input_channels, output_channels, stride=1, activate_before_residual=False):
        super().__init__()
        layer = []
        if activate_before_residual:
            self.pre_act = nn.Sequential(
                #BatchNorm2d(input_channels),
                relu()
            )
        else:
            self.pre_act = nn.Identity()
            #layer.append(BatchNorm2d(input_channels))
            layer.append(relu())
        layer.append(conv3x3(input_channels, output_channels, stride))
        #layer.append(BatchNorm2d(output_channels))
        layer.append(relu())
        layer.append(conv3x3(output_channels, output_channels))

        if stride >= 2 or input_channels != output_channels:
            self.identity = nn.Conv2d(input_channels, output_channels, 1, stride, bias=False)
        else:
            self.identity = nn.Identity()

        self.layer = nn.Sequential(*layer)

    def forward(self, x):
        x = self.pre_act(x)
        return self.identity(x) + self.layer(x)

class WRN2(nn.Module):
    """ WRN28-width with leaky relu (negative slope is 0.1)"""
    def __init__(self,width):
        super().__init__()
        self.init_conv = conv3x3(3, 16)

        filters = [16, 16*width, 32*width, 64*width]

        unit1 = [residual(filters[0], filters[1], activate_before_residual=True)] + [residual(filters[1], filters[1]) for _ in range(1, 4)]
        self.unit1 = nn.Sequential(*unit1)

        unit2 = [residual(filters[1], filters[2], 2)] + [residual(filters[2], filters[2]) for _ in range(1, 4)]
        self.unit2 = nn.Sequential(*unit2)

        unit3 = [residual(filters[2], filters[3], 2)] + [residual(filters[3], filters[3]) for _ in range(1, 4)]
        self.unit3 = nn.Sequential(*unit3)

        #self.unit4 = nn.Sequential(*[BatchNorm2d(filters[3]), relu(), nn.AdaptiveAvgPool2d(1)])
        self.unit4 = nn.Sequential(*[relu(), nn.AdaptiveAvgPool2d(1)])
        self.dropout1 = torch.nn.Dropout(0.5)


    def forward(self, x):

        x = self.init_conv(x)
        x = self.unit1(x)
        x = self.unit2(x)
        x = self.unit3(x)
        x = self.unit4(x)
        if x.shape[0]!=1:
            x = torch.squeeze(x)
        else:
            x = torch.squeeze(x)
            x = torch.unsqueeze(x, 0)
        x = F.normalize(x, p=2, dim=1)

        return x

"""forward all images"""

import torch
import numpy as np

def forward_all_images(model, device, all_img):
    batch_size = Facnet_config['batch_size']

    # Move all_img to the device directly if it is not already a tensor
    if isinstance(all_img, np.ndarray):
        all_img = torch.from_numpy(all_img).to(device=device, dtype=torch.float)
    else:
        all_img = all_img.to(device=device, dtype=torch.float)

    all_features = []
    for i in range(0, all_img.shape[0], batch_size):
        if i + batch_size >= all_img.shape[0]:
            data = all_img[i:].to(device)
        else:
            data = all_img[i:i + batch_size].to(device)

        # Forward pass through the model
        features1 = model(data)
        all_features.append(features1.cpu().detach().numpy())

    all_features = np.concatenate(all_features, axis=0)

    return all_features

"""KNC"""

# -*- coding: utf-8 -*-
"""
Created on Mon Mar 23 22:01:24 2020

@author: abdelhamid
"""
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics import accuracy_score

def KNC(clusters_centers, clusters_labels,test_features,labels_test,sigma):

    L = Facnet_config['L']
    D  = euclidean_distances(test_features,clusters_centers)
    D  = np.square(D)
    I  = np.argsort(D,axis=1)
    D1 = np.sort(D,axis=1)

    D2  = -(1/(2*sigma ))*D1
    D2  = np.exp(D1)

    label_pred = np.zeros(I.shape[0])
    for i in range(I.shape[0]):
        cl = clusters_labels[I[i,:L]]
        X  = np.zeros(len(Facnet_config['list_classes']))
        for j in Facnet_config['list_classes']:
            inds = np.where(cl==j)[0]
            X[j] = np.sum(D2[i,inds])

        label_pred[i] = np.argmax(X/X.sum())

    acc = accuracy_score(labels_test, label_pred)
    return labels_test,acc

"""distillation loss"""

import torch.nn.functional as F

def distillation_loss(student_output, teacher_output, labels, T=2.0, alpha=0.5):  # Adjust alpha to give more weight to distillation
    if len(labels.shape) > 1:
        labels = torch.argmax(labels, dim=1)
    distillation_loss = F.kl_div(
        F.log_softmax(student_output / T, dim=1),
        F.softmax(teacher_output / T, dim=1),
        reduction='batchmean'
    ) * (T * T)
    ce_loss = F.cross_entropy(student_output, labels)
    return alpha * ce_loss + (1.0 - alpha) * distillation_loss
import random
"""Replay Buffer"""


class ReplayBuffer:
    def __init__(self, buffer_size=500):
        self.buffer_size = buffer_size
        self.buffer = []

    def add(self, x, y):
        if len(self.buffer) >= self.buffer_size:
            self.buffer.pop(random.randint(0, len(self.buffer) - 1))
        self.buffer.append((x, y))

    def sample(self, batch_size):
        if len(self.buffer) < batch_size:
            batch_size = len(self.buffer)
        batch = random.sample(self.buffer, batch_size)
        x, y = zip(*batch)
        return torch.cat(x), torch.cat(y)

    def __len__(self):
        return len(self.buffer)


# Elastic Weight Consolidation (EWC) implementation
class EWC:
    def __init__(self, model, dataloader, device, importance=1000):
        self.model = model
        self.importance = importance
        self.device = device
        self.params = {n: p for n, p in model.named_parameters() if p.requires_grad}
        self.fisher_matrix = self.calculate_fisher_matrix(dataloader)

    def calculate_fisher_matrix(self, dataloader):
        fisher_matrix = {}
        for n, p in self.params.items():
            fisher_matrix[n] = torch.zeros_like(p)

        self.model.eval()
        for data, target in dataloader:
            data, target = data.to(self.device), target.to(self.device)
            output = self.model(data)
            loss = F.cross_entropy(output, target)
            self.model.zero_grad()
            loss.backward()

            for n, p in self.params.items():
                fisher_matrix[n] += p.grad ** 2

        for n in fisher_matrix:
            fisher_matrix[n] /= len(dataloader)

        return fisher_matrix

    def penalty(self, model):
        loss = 0
        for n, p in model.named_parameters():
            if p.requires_grad:
                loss += (self.fisher_matrix[n] * (p - self.params[n]).pow(2)).sum()
        return self.importance * loss


"""learning function"""

def learning_function(model, old_model, optimizer, device, images_train, labels_train, images_test, labels_test, increment, replay_buffer,ewc):
    ''' #################################################  initialization  ################################################### '''
    Epochs = Facnet_config['Epochs']
    nb_batches = Facnet_config['nb_batches']
    alpha = Facnet_config['alpha']

    train_acc, test_acc, loss = [], [], []

    ''' #################################################  Initial model testing  ################################################### '''
    train_features = forward_all_images(model, device, images_train)
    test_features = forward_all_images(model, device, images_test)

    clusters_centers, clusters_labels, samples_indices, sigma = cluster_training(train_features, labels_train.cpu().numpy())
    CB = class_batch(clusters_centers, clusters_labels, samples_indices)

    _, train_acc_1 = test_model(train_features, labels_train.cpu().numpy(), train_features, labels_train.cpu().numpy())
    _, test_acc_1 = test_model(train_features, labels_train.cpu().numpy(), test_features, labels_test.cpu().numpy())

    train_acc.append(train_acc_1)
    test_acc.append(test_acc_1)


    print("   #######################  Train Epoch: {} train_acc: {:0.4f}  test_acc: {:0.4f} ###################       ".format(0, train_acc[-1], test_acc[-1]))

    ''' #################################################### Main learning block #################################################### '''
    for epoch in range(1, Epochs + 1):
        model.train()
        indices = np.arange(clusters_centers.shape[0])
        for s in range(clusters_centers.shape[0]):
            batch_inds, batch_label, chosen_clusters = CB.construct_batch(s)
            targets = images_train[batch_inds].to(device)  # Move to device
            batch_label = torch.from_numpy(batch_label).to(device=device, dtype=torch.float)
            chosen_clusters = torch.from_numpy(chosen_clusters).to(device=device, dtype=torch.float)
            # Include samples from the replay buffer
            if replay_buffer and len(replay_buffer) > 0:
                replay_images, replay_labels = replay_buffer.sample(len(targets))
                targets = torch.cat([targets, replay_images.to(device)])
                batch_label = torch.cat([batch_label, replay_labels.to(device)])

            # Forward pass for both new model and old model
            outputs = model(targets)
            with torch.no_grad():
                old_outputs = old_model(targets)

            # Compute distillation loss and EWC penalty
            loss = distillation_loss(outputs, old_outputs, batch_label, T=2.0, alpha=alpha)
            if ewc:
                loss += ewc.penalty(model)

            # Backpropagation and optimization
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()  # Reset gradients after each step
        ''' #################################################### Update Replay Buffer ################################################ '''
        if replay_buffer:
            # Add current training samples to the replay buffer
            for i in range(len(images_train)):
                replay_buffer.add(images_train[i].cpu(), labels_train[i].cpu())

        ''' #################################################### Forward the data #################################################### '''
        train_features = forward_all_images(model, device, images_train)
        test_features = forward_all_images(model, device, images_test)

        ''' #################################################### Clustering ########################################################## '''
        clusters_centers, clusters_labels, samples_indices, sigma = cluster_training(train_features, labels_train.cpu().numpy())
        CB = class_batch(clusters_centers, clusters_labels, samples_indices)

        ''' #################################################### Testing with KNN #################################################### '''
        _, train_acc_1 = test_model(train_features, labels_train.cpu().numpy(), train_features, labels_train.cpu().numpy())
        _, test_acc_1 = test_model(train_features, labels_train.cpu().numpy(), test_features, labels_test.cpu().numpy())

        train_acc.append(train_acc_1)
        test_acc.append(test_acc_1)

        torch.save(model.state_dict(), r"D:/Deep-metric/continual_learning/models/model-"+str(increment)+".pth")
        print("   #######################  Train Epoch: {} train_acc: {:0.4f}  test_acc: {:0.4f} ###################       ".format(epoch, train_acc[-1], test_acc[-1]))

    train_acc = np.stack(train_acc)
    test_acc = np.stack(test_acc)

    np.save(f'D:/Deep-metric/continual_learning/data{increment}_train_acc.npy', train_acc)
    np.save(f'D:/Deep-metric/continual_learning/data{increment}_test_acc.npy', test_acc)

"""loss function"""

# -*- coding: utf-8 -*-
"""
Created on Mon Jan 20 23:06:38 2020

@author: abdelhamid
"""
import torch
from torch.autograd import Function

class loss_function(Function):
    def __init__(self, alpha):
        self.alpha  = alpha
        self.M      = Facnet_config['M']
        self.D      = Facnet_config['D']
        self.pdist  = torch.nn.PairwiseDistance(p=2)

        self.epsilon      = Facnet_config['epsilon']

    def mean_sigma(self, output, batch_label,chosen_clusters,device):

        sigma         = torch.zeros(chosen_clusters.shape[0]).to(device)
        mean_clusters = torch.zeros((chosen_clusters.shape[0],output.shape[1])).to(device)
        for i in range(chosen_clusters.shape[0]):
            inds               = torch.where((batch_label[:,0]==chosen_clusters[i,0]) & (batch_label[:,1]==chosen_clusters[i,1]) )[0]
            mean_clusters[i,:] = output[inds].mean(0)
            sigma[i]           = self.pdist(output[inds].mean(0),output[inds]).pow(2).mean(0)

        sigma = sigma.mean(0)
        return sigma, mean_clusters

    def forward(self, output, batch_label,chosen_clusters,device):

        sigma, mean_clusters = self.mean_sigma(output, batch_label,chosen_clusters,device)
        loss1 = torch.zeros(output.shape[0]).to(device)
        for s in range(output.shape[0]):
            dis        = -(1/(2*sigma.pow(2)))*self.pdist(output[s],mean_clusters).pow(2)
            inds1      = torch.where((chosen_clusters[:,0]==batch_label[s,0]) & (chosen_clusters[:,1]==batch_label[s,1]) )[0]
            inds2      = torch.where(chosen_clusters[:,0]!=batch_label[s,0])[0]

            num        = torch.exp(dis[inds1]-self.alpha)
            den        = torch.exp(dis[inds2]).sum(0)
            loss1[s]   = -torch.log(num/(den+self.epsilon) + self.epsilon)

        loss2 = torch.clamp(loss1, min = 0.0)
        loss  = loss2.mean(0)

        return loss

"""test model"""

# -*- coding: utf-8 -*-
"""
Created on Wed Oct 23 15:21:58 2019

@author: abdelhamid
"""
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics import accuracy_score
def test_model(train_features, labels_train,test_features,labels_test):
    K                   = Facnet_config['K']

    D  = euclidean_distances(test_features,train_features)
    D  = np.square(D)
    I  = np.argsort(D,axis=1)
    Conf1     = KNN_conf(I[:,1:K+1], labels_train)
    Conf      = np.max(Conf1,axis=1)
    label_pred = np.argmax(Conf1,axis=1)
    acc = accuracy_score(labels_test, label_pred)

    return label_pred,acc
def KNN_conf(mat, labels_train):

    conf_vector1 = np.zeros((mat.shape[0],len(Facnet_config['list_classes'])))
    for i in range(mat.shape[0]):
        for c,j in enumerate(Facnet_config['list_classes']):
            conf_vector1[i,c] = np.where(labels_train[mat[i,:]]==j)[0].shape[0]/mat.shape[1]

    return conf_vector1

"""test model with distillation"""

def test_model_with_distillation(model, old_model, test_loader, device, T=2):
    model.eval()
    old_model.eval()

    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)

            # Forward pass through both models
            outputs = model(images)
            old_outputs = old_model(images)

            # Evaluate based on distillation loss
            distillation_loss_value = distillation_loss(outputs, old_outputs, labels, T=T)

            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    accuracy = 100. * correct / total
    print(f'Test Accuracy: {accuracy}%')
    return accuracy

"""**Catastrophic Forgetting**:

In incremental learning, one of the challenges is catastrophic forgetting, where the model forgets what it learned about previous classes (like 0 and 1 in this case) as it learns about new classes (like 2 and 3). We should ensure that our model uses techniques to prevent this forgetting.

Methods we can use:
1. EWC

2.ReplayBuffer

3. Knowledge Distillation
"""

import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import numpy as np
import math
import torch.nn.functional as F

# Define the transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Mean and std values for CIFAR-10
])

# Download CIFAR-10 training and testing datasets
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

# Number of classes and subsets (5 sets, each with 2 classes)
num_classes = 10
num_sets = num_classes // 2
num_samples_per_class = 500

# Create arrays to store the indices and class counts
train_indices = [np.array([]) for _ in range(num_sets)]
test_indices = [np.array([]) for _ in range(num_sets)]
class_counts = {i: 0 for i in range(num_classes)}

# Loop through the dataset and store indices for each class in train and test sets
for idx, (_, label) in enumerate(train_dataset):
    set_idx = math.floor(label / 2)
    if class_counts[label] < num_samples_per_class:
        train_indices[set_idx] = np.append(train_indices[set_idx], idx)
        class_counts[label] += 1

for idx, (_, label) in enumerate(test_dataset):
    set_idx = label // 2
    test_indices[set_idx] = np.append(test_indices[set_idx], idx)

# Create subsets of the dataset for each set of two classes
train_dataset_subsets = [Subset(train_dataset, train_indices[i].astype(int).tolist()) for i in range(num_sets)]
test_dataset_subsets = [Subset(test_dataset, test_indices[i].astype(int).tolist()) for i in range(num_sets)]

# Helper function to fetch one batch of data from a DataLoader
def get_images_and_labels(dataloader, device):
    images, labels = next(iter(dataloader))  # Get the first batch
    images, labels = images.to(device), labels.to(device)  # Move to GPU
    return images, labels

##########################################################################################
## ------------------------------------- Creating the model-----------------------------##
##########################################################################################
learning_rate = 0.001  # Example learning rate
width = 10  # Example width parameter for your WRN2 model

model = WRN2(width)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device=device, dtype=torch.float)
model.eval()

##########################################################################################
## ---------------------------------- Creating the optimizer----------------------------##
##########################################################################################
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

##########################################################################################
## --------------------------- Incremental Learning Procedure---------------------------##
##########################################################################################
# Initialize accumulators for test sets up to the current increment
accumulated_test_indices = np.array([])
accumulated_train_indices = np.array([])  # Track accumulated training indices
all_train_acc = []
# Initialize Replay Buffer
replay_buffer = ReplayBuffer()  # Adjust the capacity as needed
def load_pretrained_model(model_path, model, device):
    model.load_state_dict(torch.load(model_path, weights_only=True))
    model = model.to(device)
    return model

# Use the learning function in your incremental learning loop
for increment in range(num_sets):
    Facnet_config['list_classes'] = [increment * 2, increment * 2 + 1]
    print(f"Training on Increment {increment + 1} - Classes: {2 * increment}, {2 * increment + 1}")

    # Load the old model for knowledge distillation
    if increment > 0:
        old_model = WRN2(width)
        old_model = load_pretrained_model("D:/Deep-metric/continual_learning/models/model-"+str(increment-1)+".pth", old_model, device)
    else:
        old_model = WRN2(width)  # First time, no distillation from old model
    old_model = old_model.to(device)

    current_train_indices = train_indices[increment]
    current_train_loader = DataLoader(Subset(train_dataset, current_train_indices.astype(int).tolist()), batch_size=64, shuffle=True)

    accumulated_test_indices = np.concatenate([accumulated_test_indices, test_indices[increment]])
    current_test_loader = DataLoader(Subset(test_dataset, accumulated_test_indices.astype(int).tolist()), batch_size=64, shuffle=False)
    # Initialize EWC for each increment
    ewc = EWC(old_model, current_train_loader, device) if increment > 0 else None
    images_train, labels_train = get_images_and_labels(current_train_loader, device)
    images_test, labels_test = get_images_and_labels(current_test_loader, device)
    # Train the model with distillation and replay buffer
    learning_function(model, old_model, optimizer, device, images_train, labels_train, images_test, labels_test, increment,replay_buffer,ewc)

    # Test model
    test_accuracy = test_model_with_distillation(model, old_model, current_test_loader, device)
    print(f'Increment {increment} Test Accuracy: {test_accuracy}')

